{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n","\n","\n","\n","<h1>Multiple Input and Output Channels</h1> \n"]},{"cell_type":"markdown","metadata":{},"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn on Multiple Input and Multiple Output Channels.</h5>    \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","# Table of Contents\n","In this lab, you will study convolution and review how the different operations change the relationship between input and output.\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"#ref0\">Multiple Output Channels </a></li>\n","\n","<li><a href=\"#ref1\">Multiple Inputs</a></li>\n","<li><a href=\"#ref2\">Multiple Input and Multiple Output Channels </a></li>\n","<li><a href=\"#ref3\">Practice Questions </a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","metadata":{},"source":["Import the following libraries:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["import torch \n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy import ndimage, misc"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref0\"></a>\n","<h2 align=center>Multiple Output Channels </h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["In Pytroch, you can create a <code>Conv2d</code> object with multiple outputs. For each channel, a kernel is created, and each kernel performs a convolution independently. As a result, the number of outputs is equal to the number of channels. This is demonstrated in the following figure. The number 9 is convolved with three kernels: each of a different color. There are three different activation maps represented by the different colors.\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmaps.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["Symbolically, this can be represented as follows:\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmap2.png\" width=\"500,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["Create a <code>Conv2d</code> with three channels:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)"]},{"cell_type":"markdown","metadata":{},"source":["Pytorch randomly assigns values to each kernel. However, use kernels that have  been developed to detect edges:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n","Gy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n","\n","conv1.state_dict()['weight'][0][0]=Gx\n","conv1.state_dict()['weight'][1][0]=Gy\n","conv1.state_dict()['weight'][2][0]=torch.ones(3,3)"]},{"cell_type":"markdown","metadata":{},"source":["Each kernel has its own bias, so set them all to zero:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\n","conv1.state_dict()['bias']"]},{"cell_type":"markdown","metadata":{},"source":["Print out each kernel: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["for x in conv1.state_dict()['weight']:\n","    print(x)"]},{"cell_type":"markdown","metadata":{},"source":["Create an input <code>image</code> to represent the input X:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["image=torch.zeros(1,1,5,5)\n","image[0,0,:,2]=1\n","image"]},{"cell_type":"markdown","metadata":{},"source":["Plot it as an image: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Perform convolution using each channel: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["out=conv1(image)"]},{"cell_type":"markdown","metadata":{},"source":["The result is a 1x3x3x3 tensor. This represents one sample with three channels, and each channel contains a 3x3 image.  The same rules that govern the shape of each image were discussed in the last section.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["out.shape"]},{"cell_type":"markdown","metadata":{},"source":["Print out each channel as a tensor or an image: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["for channel,image in enumerate(out[0]):\n","    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n","    print(image)\n","    plt.title(\"channel {}\".format(channel))\n","    plt.colorbar()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Different kernels can be used to detect various features in an image. You can see that the first channel fluctuates, and the second two channels produce a constant value. The following figure summarizes the process:\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2outputsgray.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["If you use a different image, the result will be different: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["image1=torch.zeros(1,1,5,5)\n","image1[0,0,2,:]=1\n","print(image1)\n","plt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In this case, the second channel fluctuates, and the first and the third channels produce a constant value.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["out1=conv1(image1)\n","for channel,image in enumerate(out1[0]):\n","    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n","    print(image)\n","    plt.title(\"channel {}\".format(channel))\n","    plt.colorbar()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The following figure summarizes the process:\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2ouputsgray2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref1\"></a>\n","<h2 align=center>Multiple Input Channels </h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["For two inputs, you can create two kernels. Each kernel performs a convolution on its associated input channel. The resulting output is added together as shown:  \n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.22chanalsinput.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["Create an input with two channels:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["image2=torch.zeros(1,2,5,5)\n","image2[0,0,2,:]=-2\n","image2[0,1,2,:]=1\n","image2"]},{"cell_type":"markdown","metadata":{},"source":["Plot out each image: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["for channel,image in enumerate(image2[0]):\n","    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n","    print(image)\n","    plt.title(\"channel {}\".format(channel))\n","    plt.colorbar()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Create a <code>Conv2d</code> object with two inputs:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)"]},{"cell_type":"markdown","metadata":{},"source":["Assign kernel values to make the math a little easier: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\n","conv3.state_dict()['weight'][0][0]=1*Gx1\n","conv3.state_dict()['weight'][0][1]=-2*Gx1\n","conv3.state_dict()['bias'][:]=torch.tensor([0.0])"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["conv3.state_dict()['weight']"]},{"cell_type":"markdown","metadata":{},"source":["Perform the convolution:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["conv3(image2)"]},{"cell_type":"markdown","metadata":{},"source":["The following images summarize the process. The object performs Convolution.\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["Then, it adds the result: \n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref2\"></a>\n","\n","<h2>Multiple Input and Multiple Output Channels</h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["When using multiple inputs and outputs, a kernel is created for each input, and the process is repeated for each output. The process is summarized in the following image. \n","\n","There are two input channels and 3 output channels. For each channel, the input in red and purple is convolved with an individual kernel that is colored differently. As a result, there are three outputs. \n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2mulit_input_output.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["Create an example with two inputs and three outputs and assign the kernel values to make the math a little easier: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)\n","conv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n","conv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n","\n","\n","conv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])\n","conv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])\n","\n","conv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n","conv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])"]},{"cell_type":"markdown","metadata":{},"source":["For each output, there is a bias, so set them all to zero: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])"]},{"cell_type":"markdown","metadata":{},"source":["Create a two-channel image and plot the results: \n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["image4=torch.zeros(1,2,5,5)\n","image4[0][0]=torch.ones(5,5)\n","image4[0][1][2][2]=1\n","for channel,image in enumerate(image4[0]):\n","    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n","    print(image)\n","    plt.title(\"channel {}\".format(channel))\n","    plt.colorbar()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Perform the convolution:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["z=conv4(image4)\n","z"]},{"cell_type":"markdown","metadata":{},"source":["The output of the first channel is given by: \n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["The output of the second channel is given by:\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["The output of the third channel is given by: \n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_3.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref3\"></a>\n","\n","<h2>Practice Questions </h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["Use the following two convolution objects to produce the same result as two input channel convolution on imageA and imageB as shown in the following image:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["imageA=torch.zeros(1,1,5,5)\n","imageB=torch.zeros(1,1,5,5)\n","imageA[0,0,2,:]=-2\n","imageB[0,0,2,:]=1\n","\n","\n","conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n","conv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n","\n","\n","Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\n","conv5.state_dict()['weight'][0][0]=1*Gx1\n","conv6.state_dict()['weight'][0][0]=-2*Gx1\n","conv5.state_dict()['bias'][:]=torch.tensor([0.0])\n","conv6.state_dict()['bias'][:]=torch.tensor([0.0])\n","\n","conv5(imageA)+conv6(imageB)"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_1.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_2.png\" width=\"750,\" align=\"center\">\n"]},{"cell_type":"markdown","metadata":{},"source":["Double-click __here__ for the solution.\n","\n","<!-- Your answer is below:\n","conv5(imageA)+conv6(imageB)\n","-->\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["### About the Authors:  \n","[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01), [Mavis Zhou](https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) \n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","<hr>\n","\n","## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":2}
